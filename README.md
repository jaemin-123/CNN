# 1. PYTHON í•™ìŠµëª¨ë¸(ì‹¤ìˆ˜) ì •ë‹µë¥ (í•™ìŠµë°ì´í„°, ê²€ì¦ë°ì´í„°)
## í•™ìŠµì‹œ ì •ë‹µë¥ 
<img width="494" height="243" alt="image" src="https://github.com/user-attachments/assets/9fc5f1a3-b509-438b-bfd1-182c8801f916" />




# 2. FPGA ê²€ì¦ ì •ìˆ˜ ê¸°ë°˜ ì •ë‹µë¥  (10000ê°œ, 1000ê°œ)
## FPGA ë™ì‘ ê¸°ì¤€
<img width="654" height="61" alt="image" src="https://github.com/user-attachments/assets/f8b3a2a6-f2d7-4e22-a421-9b523d51b32f" />

# 3. VIVADO IMPLEMENTATION ìì›ì‚¬ìš©ëŸ‰
<img width="573" height="271" alt="image" src="https://github.com/user-attachments/assets/00e4130f-dd25-4681-86d9-c33c94a248d5" />

<img width="542" height="188" alt="image" src="https://github.com/user-attachments/assets/95fe40ef-a96c-48bd-a4f9-446b75aba645" />

# 4. VIVADO CNN CORE SIMULATION 1000ê°œ ì´ë¯¸ì§€ ì •ë‹µë¥  (Cì–¸ì–´ë‚˜ PYTHONì´ë‚˜ Cì–¸ì–´ë¡œ ëŒë¦°ê±°ë‘ ê°™ì€ì§€) 
## FPGA 1000ê°œ ì´ë¯¸ì§€ TB
- python ë³´ë‹¤ ì •í™•ë„ ë‚®ì€ì´ìœ  ì˜ˆìƒ) ë² ë¦´ë¡œê·¸ ì½”ë“œì— ê³„ì‚°í›„ ë°˜ì˜¬ë¦¼ x
<img width="301" height="88" alt="image" src="https://github.com/user-attachments/assets/fd12808e-e755-4751-94f5-7f865472dc9a" />

# 5. CNN CORE ì´ë¯¸ì§€ í•œê°œ ì¶”ë¡ ì— í•„ìš”í•œ CLOCK ìˆ˜ 
## 100MHz ê¸°ì¤€
<img width="379" height="87" alt="image" src="https://github.com/user-attachments/assets/7f563f67-7527-4e9c-9f63-faed56b3a195" />

## 125MHz ê¸°ì¤€
<img width="379" height="87" alt="image" src="https://github.com/user-attachments/assets/c0b12540-ce5a-45c1-8894-f0842de84de0" />

# 6. FPGA ê°™ì€ ì—°ì‚°ë°©ì‹ìœ¼ë¡œ ZYNQ í”„ë¡œì„¸ì„œì—ì„œ ëŒë¦° ê²°ê³¼ ë‚˜ì˜¤ëŠ” ì‹œê°„ ë° CLOCK ìˆ˜


# ğŸ“Š FPGA-based CNN Accelerator: A Post-Training Quantization (PTQ) Approach

![Methodology](https://img.shields.io/badge/Method-PTQ-blueviolet) ![Framework](https://img.shields.io/badge/Train-PyTorch_Float32-orange) ![Hardware](https://img.shields.io/badge/Inference-Verilog_FixedPoint-blue) ![Device](https://img.shields.io/badge/Device-Zynq%2FArtix-green)

## ğŸ“– Project Abstract
ë³¸ í”„ë¡œì íŠ¸ëŠ” **PTQ(Post-Training Quantization)** ê¸°ë²•ì„ í™œìš©í•˜ì—¬ PyTorchë¡œ í•™ìŠµëœ CNN ëª¨ë¸ì„ FPGA í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¡œ êµ¬í˜„í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ Float32 ì •ë°€ë„ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ **8-bit Integer ê¸°ë°˜ì˜ í•˜ë“œì›¨ì–´(Verilog)**ë¡œ ì´ì‹í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì •í™•ë„ ì°¨ì´(Accuracy Gap)ë¥¼ ë¶„ì„í•˜ê³ , **Software Simulation ê²°ê³¼ì™€ Hardware Implementation ê²°ê³¼ê°€ ë¹„íŠ¸ ë‹¨ìœ„ê¹Œì§€ ì¼ì¹˜(Bit-True)**í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ§  Quantization Strategy (PTQ)
ì‹¤ì œ ì—£ì§€ ë””ë°”ì´ìŠ¤ í™˜ê²½ì„ ê³ ë ¤í•˜ì—¬, ì¬í•™ìŠµ(QAT) ë¹„ìš©ì´ ë“¤ì§€ ì•ŠëŠ” **Post-Training Quantization** ë°©ì‹ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

| Feature | **My Approach (PTQ)** | Comparison (QAT) |
| :--- | :--- | :--- |
| **Training** | Standard Float32 Training | Quantization simulation during training |
| **Weight Conversion** | Offline Conversion (Float â†’ Int8) | Learned during training |
| **H/W Approach** | **Bit-True w/ Software Simulation** | Minimized Quantization Loss |

---

## ğŸ— System Architecture

### 1. Model Architecture (Software)
* **Input:** 28x28 Grayscale (MNIST)
* **Layers:**
  * `Conv1`: 5x5, 3ch, ReLU, MaxPool(2x2)
  * `Conv2`: 5x5, 3ch, ReLU, MaxPool(2x2)
  * `FC Layer`: 48 Inputs â†’ 10 Outputs
* **Optimization:** Bias-Free ì„¤ê³„ ë° íŒŒë¼ë¯¸í„° ê²½ëŸ‰í™”

### 2. Hardware Design (Verilog)
* **Streaming Pipeline:** Line Bufferë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  í”½ì…€ ì…ë ¥ê³¼ ë™ì‹œì— ì—°ì‚° ìˆ˜í–‰.
* **Resources:**
  * **Line Buffer:** 5x5 Window generation
  * **PE (Processing Element):** Parallel MAC Operations
  * **Safety Logic:** Valid Signal Filtering for FC Layer

---

## ğŸ“‰ Experimental Results

### 1. Training Result (Python)
PyTorchë¥¼ ì´ìš©í•œ í•™ìŠµ ê²°ê³¼, 10 Epoch ë§Œì— **Test Accuracy 97.22%**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

<img width="842" height="194" alt="image" src="https://github.com/user-attachments/assets/b22f507b-6c67-4807-8a3d-eb17b35fc61a" />

*(Fig 1. Python Training Log showing 97.22% Test Accuracy)*

### 2. Bit-True Verification (Crucial Achievement)
í•˜ë“œì›¨ì–´ ì„¤ê³„ì˜ ë¬´ê²°ì„±ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ Pythonì—ì„œ í•˜ë“œì›¨ì–´ì™€ ë™ì¼í•œ 8-bit ì œì•½ ì¡°ê±´ì„ ê±´ ì‹œë®¬ë ˆì´ì…˜(Golden Reference)ê³¼ ì‹¤ì œ FPGA ì¶œë ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.

| Environment | Precision | Accuracy | Note |
| :--- | :--- | :--- | :--- |
| **Python Baseline** | Float32 | **97.22%** | Target |
| **Python Sim (Quantized)** | Int8 | **96.60%** | **Golden Ref** |
| **FPGA Hardware** | Int8 | **96.20%** | **Implementation** |

![Python Verification](./images/py_verification.png)
*(Fig 2. Python Simulation comparing 10k set and 1k subset accuracy)*

> **Analysis:**
> * Float32(97.2%)ì™€ Int8(96.x%) ì‚¬ì´ì˜ ì°¨ì´ëŠ” PTQ ë°©ì‹ì˜ ì–‘ìí™” ì†ì‹¤(Quantization Loss) ë° í•˜ë“œì›¨ì–´ì˜ Truncation(ë²„ë¦¼) ë°©ì‹ì— ê¸°ì¸í•©ë‹ˆë‹¤.
> * **Python Sim(Int8)ê³¼ FPGA ê²°ê³¼ê°€ ì˜¤ì°¨ ë²”ìœ„ ë‚´ì—ì„œ ì¼ì¹˜**í•œë‹¤ëŠ” ê²ƒì€ Verilog ì„¤ê³„ì— ë…¼ë¦¬ì  ì˜¤ë¥˜ê°€ ì—†ìœ¼ë©° **Bit-True**í•˜ê²Œ êµ¬í˜„ë˜ì—ˆìŒì„ ì¦ëª…í•©ë‹ˆë‹¤.

### 3. FPGA Simulation & Performance
Vivado ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼, 1000ê°œì˜ Test ì´ë¯¸ì§€ì— ëŒ€í•´ **96.2%**ì˜ ì •í™•ë„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

![FPGA TB Result](./images/fpga_result.png)
*(Fig 3. FPGA Testbench Log: 96.2% Accuracy & Inference Cycles)*

* **Clock Frequency:** 125 MHz (Target)
* **Inference Latency:** **813 Cycles** / Image
* **Throughput:** ì•½ **6.5 Âµs** per Image (@125MHz)

### 4. Resource Utilization
Implementation(Post-Route) í›„ ìì› ì‚¬ìš©ëŸ‰ì…ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ë¡œì§ ì„¤ê³„ë¥¼ í†µí•´ **DSP ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”(3%)**í•˜ê³  LUT ìœ„ì£¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

![Resource Utilization](./images/resource_util.png)
*(Fig 4. Vivado Implementation Report)*

| Resource | Used | Available | Utilization % |
| :--- | :--- | :--- | :--- |
| **LUT** | 14,643 | 53,200 | **27.52%** |
| **FF** | 12,118 | 106,400 | **11.39%** |
| **DSP** | 6 | 220 | **2.73%** |
| **BRAM** | 0.5 | 140 | **<1%** |

---

## ğŸ”§ Troubleshooting & Challenges

í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ë°œìƒí•œ ì£¼ìš” ì´ìŠˆì™€ í•´ê²° ê³¼ì •ì…ë‹ˆë‹¤.

### 1. FC Layer Garbage Data Issue
* **ë¬¸ì œ:** Convolution/Poolingì„ ê±°ì¹œ ë°ì´í„°ê°€ FC Layerë¡œ ì§„ì…í•  ë•Œ ìœ íš¨í•˜ì§€ ì•Šì€ ê°’(Garbage)ì´ ì„ì—¬ ì˜¤ë‹µë¥  ìƒìŠ¹.
* **ì›ì¸:** Line Bufferì˜ ì´ˆê¸° ì±„ì›€(Filling) êµ¬ê°„ì—ì„œ Valid ì‹ í˜¸ ì œì–´ê°€ ì •ë°€í•˜ì§€ ëª»í•¨.
* **í•´ê²°:** Valid ì‹ í˜¸ê°€ Window ë‚´ ìœ íš¨ ë°ì´í„°ê°€ ê½‰ ì°¼ì„ ë•Œë§Œ ì •í™•íˆ Highê°€ ë˜ë„ë¡ **Control Logicì„ ì¬ì„¤ê³„**í•˜ì—¬ FC Layerë¡œ ê¹¨ë—í•œ ë°ì´í„°ë§Œ ì „ë‹¬.

### 2. Vivado Optimization (Resource ~0%)
* **ë¬¸ì œ:** Behavioral Simulationì€ ì •ìƒì´ë‚˜, Implementation ì‹œ íšŒë¡œê°€ í†µì§¸ë¡œ ì‚­ì œë˜ì–´ ë¦¬ì†ŒìŠ¤ê°€ 0ì— ìˆ˜ë ´.
* **ì›ì¸:** FC Layer ì…ë ¥ë¶€ì—ì„œ ë°ì´í„° ê°œìˆ˜(48ê°œ)ë¥¼ ë„ˆë¬´ ì—„ê²©í•˜ê²Œ ì²´í¬í•˜ëŠ” ì¹´ìš´í„° ë¡œì§ ë•Œë¬¸ì— í•©ì„± íˆ´ì´ "ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë¡œì§"ìœ¼ë¡œ ì˜¤íŒí•¨.
* **í•´ê²°:** ì—„ê²©í•œ ì¹´ìš´í„° ì¡°ê±´ ëŒ€ì‹  **Data-driven(Valid ì‹ í˜¸ ê¸°ë°˜)** ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë¥¼ ì™„í™”(Relaxation)í•˜ì—¬ ì •ìƒ í•©ì„± ìœ ë„.

### 3. Truncation Bias Analysis
* **í˜„ìƒ:** ë°˜ì˜¬ë¦¼(Rounding)ì„ ì ìš©í•˜ì§€ ì•Šì•„ ë¯¸ì„¸í•œ ì •í™•ë„ í•˜ë½(ì•½ 0.5%) ê´€ì¸¡.
* **ê²°ì •:** FPGA ë¦¬ì†ŒìŠ¤ ì ˆì•½ê³¼ íƒ€ì´ë° ë§ˆì§„ í™•ë³´ë¥¼ ìœ„í•´ Rounding Logicì„ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , **Truncation(ë²„ë¦¼)** ë°©ì‹ì„ ìœ ì§€í•˜ê³  ì´ë¥¼ í•˜ë“œì›¨ì–´ íŠ¹ì„±ìœ¼ë¡œ ìˆ˜ìš©í•¨.

---

## ğŸš€ How to Run

### Python (Training & Hex Gen)
```bash
# Train Model
python _11_train_convnet.py --num_epochs 10

# Export Weights (Float -> Int8 Hex)
python export_weights.py
