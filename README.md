# ğŸ“Š FPGA-based CNN Accelerator: A Post-Training Quantization (PTQ) Approach

![Methodology](https://img.shields.io/badge/Method-PTQ-blueviolet) ![Framework](https://img.shields.io/badge/Train-PyTorch_Float32-orange) ![Hardware](https://img.shields.io/badge/Inference-Verilog_FixedPoint-blue) ![Device](https://img.shields.io/badge/Device-Zynq%2FArtix-green)

## ğŸ“– Project Abstract
ë³¸ í”„ë¡œì íŠ¸ëŠ” **PTQ(Post-Training Quantization)** ê¸°ë²•ì„ í™œìš©í•˜ì—¬ PyTorchë¡œ í•™ìŠµëœ CNN ëª¨ë¸ì„ FPGA í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¡œ êµ¬í˜„í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ Float32 ì •ë°€ë„ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ **8-bit Integer ê¸°ë°˜ì˜ í•˜ë“œì›¨ì–´(Verilog)**ë¡œ ì´ì‹í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì •í™•ë„ ì°¨ì´(Accuracy Gap)ë¥¼ ë¶„ì„í•˜ê³ , **Software Simulation ê²°ê³¼ì™€ Hardware Implementation ê²°ê³¼ê°€ ë¹„íŠ¸ ë‹¨ìœ„ê¹Œì§€ ì¼ì¹˜(Bit-True)**í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ§  Quantization Strategy (PTQ)
ì‹¤ì œ ì—£ì§€ ë””ë°”ì´ìŠ¤ í™˜ê²½ì„ ê³ ë ¤í•˜ì—¬, ì¬í•™ìŠµ(QAT) ë¹„ìš©ì´ ë“¤ì§€ ì•ŠëŠ” **Post-Training Quantization** ë°©ì‹ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

| Feature | **My Approach (PTQ)** | Comparison (QAT) |
| :--- | :--- | :--- |
| **Training** | Standard Float32 Training | Quantization simulation during training |
| **Weight Conversion** | Offline Conversion (Float â†’ Int8) | Learned during training |
| **H/W Approach** | **Bit-True w/ Software Simulation** | Minimized Quantization Loss |

---

## ğŸ— System Architecture

### 1. Model Architecture (Software)
* **Input:** 28x28 Grayscale (MNIST)
* **Layers:**
  * `Conv1`: 5x5, 3ch, ReLU, MaxPool(2x2)
  * `Conv2`: 5x5, 3ch, ReLU, MaxPool(2x2)
  * `FC Layer`: 48 Inputs â†’ 10 Outputs
* **Optimization:** Bias-Free ì„¤ê³„ ë° íŒŒë¼ë¯¸í„° ê²½ëŸ‰í™”

### 2. Hardware Design (Verilog)
* **Streaming Pipeline:** Line Bufferë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ì§€ ì•Šê³  í”½ì…€ ì…ë ¥ê³¼ ë™ì‹œì— ì—°ì‚° ìˆ˜í–‰.
* **Resources:**
  * **Line Buffer:** 5x5 Window generation
  * **PE (Processing Element):** Parallel MAC Operations
  * **Safety Logic:** Valid Signal Filtering for FC Layer

---

## ğŸ“‰ Experimental Results

### 1. Training Result (Python)
PyTorchë¥¼ ì´ìš©í•œ í•™ìŠµ ê²°ê³¼, 10 Epoch ë§Œì— **Test Accuracy 97.22%**ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

<img width="842" height="194" alt="image" src="https://github.com/user-attachments/assets/b22f507b-6c67-4807-8a3d-eb17b35fc61a" />

*(Fig 1. Python Training Log showing 97.22% Test Accuracy)*

### 2. Bit-True Verification (Crucial Achievement)
í•˜ë“œì›¨ì–´ ì„¤ê³„ì˜ ë¬´ê²°ì„±ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ Pythonì—ì„œ í•˜ë“œì›¨ì–´ì™€ ë™ì¼í•œ 8-bit ì œì•½ ì¡°ê±´ì„ ê±´ ì‹œë®¬ë ˆì´ì…˜(Golden Reference)ê³¼ ì‹¤ì œ FPGA ì¶œë ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.

| Environment | Precision | Accuracy | Note |
| :--- | :--- | :--- | :--- |
| **Python Baseline** | Float32 | **97.22%** | Target |
| **Python Sim (Quantized)** | Int8 | **96.60%** | **Golden Ref** |
| **FPGA Hardware** | Int8 | **96.20%** | **Implementation** |

<img width="654" height="61" alt="image" src="https://github.com/user-attachments/assets/f8b3a2a6-f2d7-4e22-a421-9b523d51b32f" />

<img width="301" height="88" alt="image" src="https://github.com/user-attachments/assets/fd12808e-e755-4751-94f5-7f865472dc9a" />

*(Fig 2. Python Simulation comparing 10k set and 1k subset accuracy)*

> **Detailed Analysis:**
> 1.  **Quantization Loss (97.2% â†’ 96.6%):** >     * ì‹¤ìˆ˜(Float32)ë¥¼ 8-bit ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•œ ì¼ë°˜ì ì¸ í•´ìƒë„ ì†ì‹¤ì…ë‹ˆë‹¤.
> 2.  **Rounding vs Truncation (96.6% â†’ 96.2%):**
>     * Python ì‹œë®¬ë ˆì´ì…˜ì€ ì •ìˆ˜ ë³€í™˜ ì‹œ **ë°˜ì˜¬ë¦¼(Round-to-nearest)**ì„ ìˆ˜í–‰í–ˆìœ¼ë‚˜, FPGA í•˜ë“œì›¨ì–´ëŠ” ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ì„ ìœ„í•´ **ë²„ë¦¼(Truncation)** ë°©ì‹ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.
>     * ì´ë¡œ ì¸í•´ FPGA ê²°ê³¼ê°€ Python Int8 ì‹œë®¬ë ˆì´ì…˜ ëŒ€ë¹„ ì•½ **0.4%** ë‚®ê²Œ ì¸¡ì •ë˜ì—ˆìœ¼ë‚˜, ì´ëŠ” ì„¤ê³„ ì˜ë„ì— ë¶€í•©í•˜ëŠ” í—ˆìš© ê°€ëŠ¥í•œ ì˜¤ì°¨ ë²”ìœ„ì…ë‹ˆë‹¤.

### 3. FPGA Simulation & Performance
Vivado ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼, 1000ê°œì˜ Test ì´ë¯¸ì§€ì— ëŒ€í•´ **96.2%**ì˜ ì •í™•ë„ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

<img width="379" height="87" alt="image" src="https://github.com/user-attachments/assets/c0b12540-ce5a-45c1-8894-f0842de84de0" />
*(Fig 3. FPGA Testbench Log: 96.2% Accuracy & Inference Cycles)*

* **Clock Frequency:** 125 MHz (Target)
* **Inference Latency:** **813 Cycles** / Image
* **Throughput:** ì•½ **6.5 Âµs** per Image (@125MHz)

### 4. Resource Utilization
Implementation(Post-Route) í›„ ìì› ì‚¬ìš©ëŸ‰ì…ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ ë¡œì§ ì„¤ê³„ë¥¼ í†µí•´ **DSP ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”(3%)**í•˜ê³  LUT ìœ„ì£¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

<img width="573" height="271" alt="image" src="https://github.com/user-attachments/assets/00e4130f-dd25-4681-86d9-c33c94a248d5" />

<img width="542" height="188" alt="image" src="https://github.com/user-attachments/assets/95fe40ef-a96c-48bd-a4f9-446b75aba645" />
*(Fig 4. Vivado Implementation Report)*

| Resource | Used | Available | Utilization % |
| :--- | :--- | :--- | :--- |
| **LUT** | 14,643 | 53,200 | **27.52%** |
| **FF** | 12,118 | 106,400 | **11.39%** |
| **DSP** | 6 | 220 | **2.73%** |
| **BRAM** | 0.5 | 140 | **<1%** |

---

## ğŸ”§ Troubleshooting & Challenges

í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ë°œìƒí•œ ì£¼ìš” ì´ìŠˆì™€ í•´ê²° ê³¼ì •ì…ë‹ˆë‹¤.

### 1. FC Layer Garbage Data Issue
* **ë¬¸ì œ:** Convolution/Poolingì„ ê±°ì¹œ ë°ì´í„°ê°€ FC Layerë¡œ ì§„ì…í•  ë•Œ ìœ íš¨í•˜ì§€ ì•Šì€ ê°’(Garbage)ì´ ì„ì—¬ ì˜¤ë‹µë¥  ìƒìŠ¹.
* **ì›ì¸:** Line Bufferì˜ ì´ˆê¸° ì±„ì›€(Filling) êµ¬ê°„ì—ì„œ Valid ì‹ í˜¸ ì œì–´ê°€ ì •ë°€í•˜ì§€ ëª»í•¨.
* **í•´ê²°:** Valid ì‹ í˜¸ê°€ Window ë‚´ ìœ íš¨ ë°ì´í„°ê°€ ê½‰ ì°¼ì„ ë•Œë§Œ ì •í™•íˆ Highê°€ ë˜ë„ë¡ **Control Logicì„ ì¬ì„¤ê³„**í•˜ì—¬ FC Layerë¡œ ê¹¨ë—í•œ ë°ì´í„°ë§Œ ì „ë‹¬.

### 2. Vivado Optimization (Resource ~0%)
* **ë¬¸ì œ:** Behavioral Simulationì€ ì •ìƒì´ë‚˜, Implementation ì‹œ íšŒë¡œê°€ í†µì§¸ë¡œ ì‚­ì œë˜ì–´ ë¦¬ì†ŒìŠ¤ê°€ 0ì— ìˆ˜ë ´.
* **ì›ì¸:** FC Layer ì…ë ¥ë¶€ì—ì„œ ë°ì´í„° ê°œìˆ˜(48ê°œ)ë¥¼ ë„ˆë¬´ ì—„ê²©í•˜ê²Œ ì²´í¬í•˜ëŠ” ì¹´ìš´í„° ë¡œì§ ë•Œë¬¸ì— í•©ì„± íˆ´ì´ "ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë¡œì§"ìœ¼ë¡œ ì˜¤íŒí•¨.
* **í•´ê²°:** ì—„ê²©í•œ ì¹´ìš´í„° ì¡°ê±´ ëŒ€ì‹  **Data-driven(Valid ì‹ í˜¸ ê¸°ë°˜)** ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë¥¼ ì™„í™”(Relaxation)í•˜ì—¬ ì •ìƒ í•©ì„± ìœ ë„.

### 3. Truncation Bias Analysis
* **í˜„ìƒ:** ë°˜ì˜¬ë¦¼(Rounding)ì„ ì ìš©í•˜ì§€ ì•Šì•„ ë¯¸ì„¸í•œ ì •í™•ë„ í•˜ë½(ì•½ 0.5%) ê´€ì¸¡.
* **ê²°ì •:** FPGA ë¦¬ì†ŒìŠ¤ ì ˆì•½ê³¼ íƒ€ì´ë° ë§ˆì§„ í™•ë³´ë¥¼ ìœ„í•´ Rounding Logicì„ ì¶”ê°€í•˜ëŠ” ëŒ€ì‹ , **Truncation(ë²„ë¦¼)** ë°©ì‹ì„ ìœ ì§€í•˜ê³  ì´ë¥¼ í•˜ë“œì›¨ì–´ íŠ¹ì„±ìœ¼ë¡œ ìˆ˜ìš©í•¨.

### 4. Timing Violation at 125MHz (Intra-Module Pipelining)
* **ë¬¸ì œ:** ëª©í‘œ ë™ì‘ ì£¼íŒŒìˆ˜ì¸ 125MHz (Period 8ns)ì—ì„œ Conv PE ë° FC Layer ë‚´ë¶€ì˜ ê¸´ ì¡°í•© íšŒë¡œ(MAC ì—°ì‚° ë“±)ë¡œ ì¸í•´ Setup Time Violation ë°œìƒ.
* **ì›ì¸:** ë ˆì´ì–´ ê°„ ì—°ê²°ì´ ì•„ë‹Œ, **ë‹¨ì¼ ì—°ì‚° ëª¨ë“ˆ(PE, FC Unit) ë‚´ë¶€**ì˜ Critical Pathê°€ í•œ í´ëŸ­ ì£¼ê¸°ë¥¼ ì´ˆê³¼í•¨.
* **í•´ê²°:** ì—°ì‚°ê¸° ë‚´ë¶€ì˜ ê³±ì…ˆê³¼ ë§ì…ˆ, í™œì„±í™” í•¨ìˆ˜(ReLU) ì‚¬ì´ì— **Pipeline Register**ë¥¼ ì‚½ì…í•˜ì—¬ Critical Pathë¥¼ ë¶„í• í•¨. ì´ë¥¼ í†µí•´ LatencyëŠ” ì†Œí­ ì¦ê°€í–ˆìœ¼ë‚˜ 125MHz ë™ì‘ íƒ€ì´ë°ì„ ì•ˆì •ì ìœ¼ë¡œ í™•ë³´í•¨.

---


# ğŸš€ FPGA-based CNN Accelerator: A Post-Training Quantization (PTQ) Approach

![Methodology](https://img.shields.io/badge/Method-PTQ-blueviolet) ![Framework](https://img.shields.io/badge/Train-PyTorch_Float32-orange) ![Hardware](https://img.shields.io/badge/Inference-Verilog_FixedPoint-blue) ![Performance](https://img.shields.io/badge/Freq-125MHz-green)

## ğŸ“– Project Abstract
ë³¸ í”„ë¡œì íŠ¸ëŠ” **PTQ(Post-Training Quantization)** ê¸°ë²•ì„ í™œìš©í•˜ì—¬ PyTorchë¡œ í•™ìŠµëœ CNN ëª¨ë¸ì„ FPGA í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ë¡œ êµ¬í˜„í•œ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.

ì¼ë°˜ì ì¸ Float32 ì •ë°€ë„ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ **8-bit Integer ê¸°ë°˜ì˜ í•˜ë“œì›¨ì–´(Verilog)**ë¡œ ì´ì‹í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì •í™•ë„ ì°¨ì´(Accuracy Gap)ë¥¼ ë¶„ì„í•˜ê³ , **Software Simulation ê²°ê³¼ì™€ Hardware Implementation ê²°ê³¼ê°€ ì˜¤ì°¨ ë²”ìœ„ ë‚´ì—ì„œ ì¼ì¹˜**í•¨ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.

íŠ¹íˆ, ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë²„í¼ì— ì €ì¥í•˜ì§€ ì•Šê³  **Streaming Pipeline** êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì—¬ Latencyë¥¼ ìµœì†Œí™”í•˜ê³  Throughputì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤.

---

## âš¡ Key Feature: Streaming Pipeline Architecture

ì´ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ì€ ë°ì´í„°ê°€ ë©ˆì¶”ì§€ ì•Šê³  íë¥´ëŠ” **Fully Pipelined Structure**ì…ë‹ˆë‹¤.

<p align="center">
  <img width="800" alt="Waveform" src="HERE_PUT_YOUR_WAVEFORM_IMAGE_LINK" />
</p>
*(Fig 1. Simulation Waveform showing the Data Flow)*

> **Waveform Analysis (The "Staircase" Effect):**
> ìœ„ íŒŒí˜•ì—ì„œ **ì‹ í˜¸ë“¤ì´ ìš°í•˜í–¥ ê³„ë‹¨(â†˜)**ì„ ê·¸ë¦¬ë©° ìˆœì°¨ì ìœ¼ë¡œ ì¼œì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> 1. `valid_in`: í”½ì…€ ë°ì´í„° ì…ë ¥ ì‹œì‘
> 2. `l1_win_valid`: Layer 1 Line Bufferê°€ ì±„ì›Œì§€ê³  ì—°ì‚° ì‹œì‘
> 3. `l1_pool_valid`: Layer 1 ê²°ê³¼ ì¶œë ¥ ë° Layer 2 ì…ë ¥
> 4. `l2_common_valid`: Layer 2 ì—°ì‚° ì‹œì‘
> 5. `fc_done`: ìµœì¢… ì¶”ë¡  ì™„ë£Œ
>
> ì´ëŠ” ë°ì´í„° ë³‘ëª©(Bottleneck) ì—†ì´ ê° ë ˆì´ì–´ê°€ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìœ ê¸°ì ìœ¼ë¡œ ë™ì‘í•˜ê³  ìˆìŒì„ ì‹œê°ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.

---

## ğŸ— System Architecture

### 1. Model & Quantization (Software)
* **Architecture:** Conv(5x5) â†’ MaxPool â†’ Conv(5x5) â†’ MaxPool â†’ FC (Bias-Free)
* **Quantization Strategy (PTQ):**
    * Weights & Activations: **8-bit Integer**
    * Scaling: Layer-wise shift operations

### 2. Hardware Design (Verilog)
* **Line Buffer Unit:** 5x5 Sliding Windowë¥¼ ì‹¤ì‹œê°„ ìƒì„±í•˜ì—¬ ë©”ëª¨ë¦¬ ì ‘ê·¼ ìµœì†Œí™”.
* **Intra-Module Pipelining:** 125MHz ë™ì‘ì„ ìœ„í•´ PE(Processing Element) ë‚´ë¶€ì˜ Critical Pathì— Pipeline Register ì‚½ì….
* **Optimization:** DSP ì‚¬ìš©ì„ ìµœì†Œí™”í•˜ê³  LUT ìœ„ì£¼ì˜ ë¡œì§ ì„¤ê³„ë¡œ ìì› íš¨ìœ¨ì„± ê·¹ëŒ€í™”.

---

## ğŸ“‰ Experimental Results

### 1. Training & Verification (Python)
PyTorch í•™ìŠµ ê²°ê³¼ **97.22%**ì˜ ì •í™•ë„ë¥¼ ë‹¬ì„±í–ˆìœ¼ë©°, FPGA ë™ì‘ì„ ëª¨ì‚¬í•œ Quantized Simulation(Int8)ì—ì„œë„ **96.60%**ì˜ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.

<p align="center">
  <img width="842" height="194" alt="Training Log" src="https://github.com/user-attachments/assets/b22f507b-6c67-4807-8a3d-eb17b35fc61a" />
</p>
*(Fig 2. Python Training Log showing 97.22% Test Accuracy)*

### 2. Accuracy Gap Analysis (Software vs Hardware)
í•˜ë“œì›¨ì–´ ì„¤ê³„ì˜ ë¬´ê²°ì„±ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ Python ì‹œë®¬ë ˆì´ì…˜(Golden Ref)ê³¼ ì‹¤ì œ FPGA ì¶œë ¥ì„ ë¹„êµí–ˆìŠµë‹ˆë‹¤.

| Environment | Precision | Accuracy | Note |
| :--- | :--- | :--- | :--- |
| **Python Baseline** | Float32 | **97.22%** | Target Accuracy |
| **Python Sim (Quantized)** | Int8 | **96.60%** | **Golden Reference** |

<p align="center">
  <img width="654" height="61" alt="Python Verification" src="https://github.com/user-attachments/assets/f8b3a2a6-f2d7-4e22-a421-9b523d51b32f" />
  <br>
  <img width="301" height="88" alt="Python Stats" src="https://github.com/user-attachments/assets/fd12808e-e755-4751-94f5-7f865472dc9a" />
</p>
*(Fig 3. Python Simulation Verification)*

> **Detailed Analysis:**
> 1.  **Quantization Loss (97.2% â†’ 96.6%):**
>     * ì‹¤ìˆ˜(Float32)ë¥¼ 8-bit ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•œ ì¼ë°˜ì ì¸ í•´ìƒë„ ì†ì‹¤ì…ë‹ˆë‹¤.
> 2.  **Rounding vs Truncation (96.6% â†’ 96.2%):**
>     * Python ì‹œë®¬ë ˆì´ì…˜ì€ ì •ìˆ˜ ë³€í™˜ ì‹œ **ë°˜ì˜¬ë¦¼(Round-to-nearest)**ì„ ìˆ˜í–‰í–ˆìœ¼ë‚˜, FPGA í•˜ë“œì›¨ì–´ëŠ” ë¦¬ì†ŒìŠ¤ íš¨ìœ¨ì„±ì„ ìœ„í•´ **ë²„ë¦¼(Truncation)** ë°©ì‹ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.
>     * ì´ë¡œ ì¸í•œ ì•½ **0.4%**ì˜ ì°¨ì´ëŠ” ì„¤ê³„ ì˜ë„ì— ë¶€í•©í•˜ëŠ” í—ˆìš© ê°€ëŠ¥í•œ ì˜¤ì°¨ ë²”ìœ„ì…ë‹ˆë‹¤.

### 3. FPGA Hardware Performance
ì‹¤ì œ FPGA Testbench ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼, 1000ê°œì˜ Test ì´ë¯¸ì§€ì— ëŒ€í•´ **96.2%**ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤.

<p align="center">
<img width="379" height="87" alt="FPGA Result" src="https://github.com/user-attachments/assets/c0b12540-ce5a-45c1-8894-f0842de84de0" />
</p>
*(Fig 4. FPGA Simulation Result: 96.2% Accuracy & Cycle Counts)*

* **Latency & Speed:**
    * **Clock Cycles:** 813 Cycles / Image
    * **Inference Time:** **6.5 Âµs** (@125MHz)

### 4. Resource Utilization
Implementation(Post-Route) ê²°ê³¼ì…ë‹ˆë‹¤. **DSPë¥¼ ë‹¨ 6ê°œ(3%)ë§Œ ì‚¬ìš©**í•˜ë©´ì„œë„ íš¨ìœ¨ì ì¸ CNN ê°€ì† ì„±ëŠ¥ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

<p align="center">
  <img width="573" height="271" alt="Resource Graph" src="https://github.com/user-attachments/assets/00e4130f-dd25-4681-86d9-c33c94a248d5" />
  <br>
  <img width="542" height="188" alt="Resource Table" src="https://github.com/user-attachments/assets/95fe40ef-a96c-48bd-a4f9-446b75aba645" />
</p>
*(Fig 5. Vivado Implementation Report)*

| Resource | Used | Available | Utilization % |
| :--- | :--- | :--- | :--- |
| **LUT** | 14,643 | 53,200 | **27.52%** |
| **FF** | 12,118 | 106,400 | **11.39%** |
| **DSP** | **6** | 220 | **2.73%** (High Efficiency) |
| **BRAM** | 0.5 | 140 | < 1% |

---

## ğŸ”§ Troubleshooting & Challenges

### 1. Timing Violation at 125MHz
* **ë¬¸ì œ:** 100MHzì—ì„œëŠ” ì •ìƒ ë™ì‘í•˜ë˜ íšŒë¡œê°€ 125MHzì—ì„œ Setup Time Violation ë°œìƒ.
* **ì›ì¸:** Conv PE ë° FC Layer ë‚´ë¶€ì˜ MAC ì—°ì‚° ê²½ë¡œ(Combinational Logic)ê°€ ë„ˆë¬´ ê¸¸ì–´ì§.
* **í•´ê²°:** ì—°ì‚°ê¸° ë‚´ë¶€(Intra-module)ì— **Pipeline Register**ë¥¼ ì¶”ê°€í•˜ì—¬ Critical Pathë¥¼ ë¶„í• (Retiming). LatencyëŠ” ì†Œí­ ì¦ê°€í–ˆìœ¼ë‚˜ **125MHz Timing Constraint**ë¥¼ ì™„ë²½í•˜ê²Œ ë§Œì¡±.

### 2. FC Layer Garbage Data Issue
* **ë¬¸ì œ:** Line Bufferê°€ ì±„ì›Œì§€ëŠ” ì´ˆê¸° êµ¬ê°„(Filling state)ì—ì„œ ë¶€ì •í™•í•œ Valid ì‹ í˜¸ë¡œ ì¸í•´ FC Layerì— ì“°ë ˆê¸° ê°’ì´ ìœ ì…ë¨.
* **í•´ê²°:** Valid ì‹ í˜¸ ìƒì„± ë¡œì§ì„ **Data-driven ë°©ì‹**ìœ¼ë¡œ ì¬ì„¤ê³„. Windowê°€ ìœ íš¨í•œ ë°ì´í„°ë¡œ ê½‰ ì°¼ì„ ë•Œë§Œ ì •í™•íˆ Validë¥¼ Highë¡œ ë„ìš°ë„ë¡ ìˆ˜ì •í•˜ì—¬ ì˜¤ë™ì‘ ë°©ì§€.

### 3. Vivado Optimization Issue (Resource ~0%)
* **ë¬¸ì œ:** Behavioral Simulationì€ ì •ìƒì´ë‚˜, Implementation ì‹œ íšŒë¡œê°€ í†µì§¸ë¡œ ì‚­ì œë˜ì–´ ë¦¬ì†ŒìŠ¤ê°€ 0ì— ìˆ˜ë ´.
* **ì›ì¸:** FC Layer ì…ë ¥ë¶€ì—ì„œ ë°ì´í„° ê°œìˆ˜(48ê°œ)ë¥¼ ë„ˆë¬´ ì—„ê²©í•˜ê²Œ ì²´í¬í•˜ëŠ” ì¹´ìš´í„° ë¡œì§ ë•Œë¬¸ì— í•©ì„± íˆ´ì´ "ë„ë‹¬ ë¶ˆê°€ëŠ¥í•œ ë¡œì§"ìœ¼ë¡œ ì˜¤íŒí•¨.
* **í•´ê²°:** ì—„ê²©í•œ ì¹´ìš´í„° ì¡°ê±´ ëŒ€ì‹  **Data-driven(Valid ì‹ í˜¸ ê¸°ë°˜)** ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë¥¼ ì™„í™”(Relaxation)í•˜ì—¬ ì •ìƒ í•©ì„± ìœ ë„.

---

## ğŸš€ How to Run

### Python
```bash
# Train Model
python _11_train_convnet.py --num_epochs 10
# Export Weights
python export_weights.py
